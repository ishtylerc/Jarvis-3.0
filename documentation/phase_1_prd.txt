# Jarvis - Product Requirements Document - Phase 1

**Phase:** 1 - Core Functionality & Basic Interaction Enhancements
**Version:** 1.0
**Date:** 2024-07-28

## 1. Goals

*   Establish a stable and robust backend infrastructure to support the existing frontend client.
*   Implement the core WebRTC communication channel between the frontend and backend, and subsequently with the OpenAI Realtime API.
*   Enable essential real-time interaction features: audio streaming (both ways) and live transcription display.
*   Implement basic conversational memory within a single session.
*   Ensure the existing frontend `App.jsx` and related components can successfully connect, interact, and disconnect via the backend.

## 2. Features

*   **F1.1: Backend Server Setup:**
    *   A basic backend server (e.g., Node.js/Express, Python/FastAPI) capable of handling HTTP requests and potentially WebSocket/WebRTC signaling.
*   **F1.2: OpenAI Token Endpoint (`/token`):**
    *   Backend endpoint that securely generates or retrieves and returns an ephemeral OpenAI API key/session token required for the frontend to authorize with the OpenAI Realtime API during WebRTC negotiation.
*   **F1.3: Backend WebRTC Handling & Proxying:**
    *   Backend logic to handle the SDP offer/answer exchange initiated by the frontend (`startSession`).
    *   Ability to manage the connection lifecycle (start, stop).
    *   Acts as a secure intermediary or directly facilitates the connection between the client and OpenAI's Realtime API endpoint.
    *   Handles streaming audio from the client mic to OpenAI.
    *   Handles streaming audio from OpenAI back to the client.
*   **F1.4: Real-time Transcription Display:**
    *   Frontend capability to receive and display transcription events (e.g., `transcription.update`, `transcription.complete`) streamed from OpenAI (likely via the data channel handled by the backend).
    *   Display should update in near real-time as the user speaks and as the AI formulates its spoken response.
*   **F1.5: Basic In-Session Memory:**
    *   Backend mechanism to store key information or conversation turns within the context of the current active session.
    *   This context should be included in subsequent requests/prompts to OpenAI during that session to maintain conversational flow. The context is lost when the session ends.

## 3. Tasks & Subtasks

*   **T1.1: Setup Backend Project:**
    *   T1.1.1: Choose backend language/framework (e.g., Node.js/Express).
    *   T1.1.2: Initialize project structure, dependency management (npm/yarn).
    *   T1.1.3: Setup basic server listener.
*   **T1.2: Implement `/token` Endpoint:**
    *   T1.2.1: Define endpoint route (`/token`).
    *   T1.2.2: Implement logic to securely obtain/generate the OpenAI ephemeral key (consider environment variables or secure config).
    *   T1.2.3: Return the key in the expected JSON format `{ client_secret: { value: '...' } }`.
    *   T1.2.4: Add basic error handling for token generation/retrieval failures.
    *   T1.2.5: Create unit/integration test for the `/token` endpoint (verify success response format, key presence, and error handling scenarios).
    *   T1.2.6: Run `/token` endpoint test and confirm pass.
*   **T1.3: Implement Backend WebRTC Logic:**
    *   T1.3.1: Setup WebRTC library/dependencies on the backend if needed (or determine if direct proxying is feasible).
    *   T1.3.2: Create endpoint/handler to receive SDP offer from the client.
    *   T1.3.3: Implement logic to forward the offer to OpenAI Realtime API and receive the SDP answer.
    *   T1.3.4: Return the SDP answer to the client. (Note: Testing this specific interaction might require mocking OpenAI or rely on E2E tests in T1.6).
    *   T1.3.5: Implement connection state management logic (active/inactive).
    *   T1.3.6: Create unit test for connection state management logic.
    *   T1.3.7: Run connection state management test and confirm pass.
    *   T1.3.8: Implement handling for audio track streaming (client -> OpenAI, OpenAI -> client). (Note: Requires E2E/manual testing in T1.6).
    *   T1.3.9: Implement handling for data channel events (client -> OpenAI, OpenAI -> client), including basic parsing and routing.
    *   T1.3.10: Create unit test for data channel event handler logic (verify correct parsing and initial processing of simulated events).
    *   T1.3.11: Run data channel event handler test and confirm pass.
    *   T1.3.12: Implement session cleanup logic triggered by client disconnect (`stopSession`).
    *   T1.3.13: Create unit test for session cleanup logic.
    *   T1.3.14: Run session cleanup test and confirm pass.
*   **T1.4: Implement Frontend Transcription Display:**
    *   T1.4.1: Modify `App.jsx` or create a new component to listen for specific transcription events from the data channel (received via the backend).
    *   T1.4.2: Maintain state for the current user transcription and AI transcription.
    *   T1.4.3: Render the transcription state in a designated area of the UI, updating dynamically.
    *   T1.4.4: Ensure clear visual distinction between user and AI transcriptions.
    *   T1.4.5: Create frontend component test for transcription display (verify state updates, dynamic rendering based on simulated events).
    *   T1.4.6: Run transcription display component test and confirm pass.
*   **T1.5: Implement Basic In-Session Memory:**
    *   T1.5.1: Design simple data structure for storing session context on the backend.
    *   T1.5.2: Update backend logic to append relevant conversation turns to the session context.
    *   T1.5.3: Modify interaction logic to inject stored context into prompts.
    *   T1.5.4: Ensure context is cleared when a session ends.
    *   T1.5.5: Create backend unit test for session memory logic (storing, retrieving, clearing context, context injection prep).
    *   T1.5.6: Run session memory logic test and confirm pass.
*   **T1.6: End-to-End (E2E) and Manual Testing:**
    *   T1.6.1: Manually connect frontend `startSession` to use the backend `/token` endpoint and SDP handling.
    *   T1.6.2: Manually/E2E test successful WebRTC connection establishment via the backend.
    *   T1.6.3: Manually/E2E test audio input/output streaming.
    *   T1.6.4: Manually/E2E test real-time transcription display for both user and AI.
    *   T1.6.5: Manually/E2E test basic conversation flow, verifying session memory usage.
    *   T1.6.6: Manually/E2E test session start/stop functionality thoroughly.
*   **T1.7: Document Phase 1 Implementation:**
    *   T1.7.1: Update `documentation/jarvis_documentation/backend_documentation.md` with details on the server setup, `/token` endpoint implementation, and backend WebRTC handling logic.
    *   T1.7.2: Update `documentation/jarvis_documentation/frontend_documentation.md` section related to real-time transcription display integration and any modifications made to `App.jsx` for backend communication.
    *   T1.7.3: Update `documentation/jarvis_documentation/jarvis_architecture.md` with a high-level diagram and description of the initial backend-frontend connection, WebRTC flow, and basic memory handling established in this phase.
    *   T1.7.4: Ensure `documentation/jarvis_documentation/README.md` provides an up-to-date overview reflecting Phase 1 completion status.

## 4. Assumptions

*   The existing frontend (`client` directory) structure and core components (`App.jsx`, `SessionControls.jsx`) will be used as the starting point.
*   Access to OpenAI API keys and understanding of the Realtime API protocol is available.
*   Basic familiarity with WebRTC concepts (SDP, ICE, Data Channels, Media Tracks) exists.

## 5. Out of Scope for Phase 1

*   Advanced agent frameworks or complex orchestration.
*   Persistent memory across sessions.
*   Function calling / Tool use.
*   Vision capabilities.
*   External access beyond the local development environment.
*   Proactive assistant features.
*   User authentication or multiple user support.
*   Advanced UI/UX refinements beyond transcription display. 