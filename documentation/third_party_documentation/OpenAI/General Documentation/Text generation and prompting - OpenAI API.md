---
title: "Text generation and prompting - OpenAI API"
source: "https://platform.openai.com/docs/guides/text?api-mode=responses"
author:
published:
created: 2025-03-20
description: "Learn how to use the OpenAI API to generate text from a prompt. Learn about message types and available text formats like JSON and Structured Outputs."
tags:
  - "clippings"
---
Learn how to prompt a model to generate text.

With the OpenAI API, you can use a [large language model](https://platform.openai.com/docs/models) to generate text from a prompt, as you might using [ChatGPT](https://chatgpt.com/). Models can generate almost any kind of text responseâ€”like code, mathematical equations, structured JSON data, or human-like prose.

Here's a simple example using the [Responses API](https://platform.openai.com/docs/api-reference/responses).

Generate text from a simple prompt

```highlighter
1
2
3
4
5
6
7
8
9

from openai import OpenAI
client = OpenAI()

response = client.responses.create(
    model="gpt-4o",
    input="Write a one-sentence bedtime story about a unicorn."
)

print(response.output_text)
```

An array of content generated by the model is in the `output` property of the response. In this simple example, we have just one output which looks like this:

```highlighter
1
2
3
4
5
6
7
8
9
10
11
12
13
14

[
    {
        "id": "msg_67b73f697ba4819183a15cc17d011509",
        "type": "message",
        "role": "assistant",
        "content": [
            {
                "type": "output_text",
                "text": "Under the soft glow of the moon, Luna the unicorn danced through fields of twinkling stardust, leaving trails of dreams for every child asleep.",
                "annotations": []
            }
        ]
    }
]
```

Note that the output often has more than one item!

Some of our [official SDKs](https://platform.openai.com/docs/libraries) include an `output_text` property on model responses for convenience, which aggregates all text outputs from the model into a single string. This may be useful as a shortcut to access text output from the model.

In addition to plain text, you can also have the model return structured data in JSON format - this feature is called [**Structured Outputs**](https://platform.openai.com/docs/guides/structured-outputs).

You can provide instructions to the model with [differing levels of authority](https://model-spec.openai.com/2025-02-12.html#chain_of_command) using the `instructions` API parameter or **message roles**.

The `instructions` parameter gives the model high-level instructions on how it should behave while generating a response, including tone, goals, and examples of correct responses. Any instructions provided this way will take priority over a prompt in the `input` parameter.

Generate text with instructions

```highlighter
1
2
3
4
5
6
7
8
9
10

from openai import OpenAI
client = OpenAI()

response = client.responses.create(
    model="gpt-4o",
    instructions="Talk like a pirate.",
    input="Are semicolons optional in JavaScript?",
)

print(response.output_text)
```

The example above is roughly equivalent to using the following input messages in the `input` array:

Generate text with messages using different roles

```highlighter
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18

from openai import OpenAI
client = OpenAI()

response = client.responses.create(
    model="gpt-4o",
    input=[
        {
            "role": "developer",
            "content": "Talk like a pirate."
        },
        {
            "role": "user",
            "content": "Are semicolons optional in JavaScript?"
        }
    ]
)

print(response.output_text)
```

Instructions versus developer messages in multi-turn conversations

The [OpenAI model spec](https://model-spec.openai.com/2025-02-12.html#chain_of_command) describes how our models give different levels of priority to messages with different roles.

| `developer` | `user` | `assistant` |
| --- | --- | --- |
| `developer` messages are instructions provided by the application developer, weighted ahead of `user` messages. | `user` messages are instructions provided by an end user, weighted behind `developer` messages. | Messages generated by the model have the `assistant` role. |

A multi-turn conversation may consist of several messages of these types, along with other content types provided by both you and the model. Learn more about [managing conversation state here](https://platform.openai.com/docs/guides/conversation-state).

## Choosing a model

A key choice to make when generating content through the API is which model you want to use - the `model` parameter of the code samples above. [You can find a full listing of available models here](https://platform.openai.com/docs/models).

### Which model should I choose?

Here are a few factors to consider when choosing a model for text generation.

- **[Reasoning models](https://platform.openai.com/docs/guides/reasoning)** generate an internal chain of thought to analyze the input prompt, and excel at understanding complex tasks and multi-step planning. They are also generally slower and more expensive to use than GPT models.
- **GPT models** are fast, cost-efficient, and highly intelligent, but benefit from more explicit instructions around how to accomplish tasks.
- **Large and small (mini) models** offer trade-offs for speed, cost, and intelligence. Large models are more effective at understanding prompts and solving problems across domains, while small models are generally faster and cheaper to use. Small models like GPT-4o mini can also be trained to excel at a specific task through [fine tuning](https://platform.openai.com/docs/guides/fine-tuning) and [distillation](https://platform.openai.com/docs/guides/distillation) of results from larger models.

When in doubt, `gpt-4o` offers a solid combination of intelligence, speed, and cost effectiveness.

## Prompt engineering

Creating effective instructions for a model to generate content is a process known as **prompt engineering**. Because the content generated from a model is non-deterministic, it is a combination of art and science to build a prompt that will generate the right kind of content from a model. You can find a more complete exploration of [prompt engineering here](https://platform.openai.com/docs/guides/prompt-engineering), but here are some general guidelines:

- Be detailed in your instructions to the model to eliminate ambiguity in how you want the model to respond.
- Provide examples to the model of the type of inputs you expect, and the type of outputs you would want for that input - this technique is called **few-shot learning**.
- When using a [reasoning model](https://platform.openai.com/docs/guides/reasoning), describe the task to be done in terms of goals and desired outcomes, rather than specific step-by-step instructions of how to accomplish a task.
- Invest in creating [evaluations (evals)](https://platform.openai.com/docs/guides/evals) for your prompts, using test data that looks like the data you expect to see in production. Due to the inherent variable results from different models, using evals to see how your prompts perform is the best way to ensure your prompts work as expected.

Iterating on prompts is often all you need to do to get great results from a model, but you can also explore [fine tuning](https://platform.openai.com/docs/guides/fine-tuning) to customize base models for a particular use case.

Now that you known the basics of text inputs and outputs, you might want to check out one of these resources next.

[

Build a prompt in the Playground

Use the Playground to develop and iterate on prompts.

](https://platform.openai.com/playground)[

Generate JSON data with Structured Outputs

Ensure JSON data emitted from a model conforms to a JSON schema.

](https://platform.openai.com/docs/guides/structured-outputs)[

Full API reference

Check out all the options for text generation in the API reference.

](https://platform.openai.com/docs/api-reference/responses)